{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "import requests\n",
    "import urllib.request\n",
    "import json\n",
    "import os, sys, re\n",
    "import logging\n",
    "from datetime import date, datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip freeze > \"requirements.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assign Notebook Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datetime_today = (datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")).replace(\":\",\"\").replace(\" \",\"_\")\n",
    "\n",
    "## API parameters\n",
    "cms_url = \"https://data.cms.gov/provider-data/api/1/metastore/schemas/dataset/items\"\n",
    "hospitals_relative_url = \"?show-reference-ids=false\"\n",
    "params=\"application/json\"\n",
    "\n",
    "## Paths parameters\n",
    "relative_path = os.path.dirname(os.getcwd())\n",
    "temp_file_path = os.path.join(relative_path,\"01_Source\",\"temp\",f\"cms_{datetime_today}.json\")\n",
    "csv_file_path = os.path.join(relative_path,\"01_Source\",\"csv\",f\"hospitals_{datetime_today}.csv\")\n",
    "previous_csv_file = os.listdir(os.path.join(relative_path,\"01_Source\",\"csv\"))[-1]\n",
    "previous_csv_file_path = os.path.join(relative_path,\"01_Source\",\"csv\",f\"{previous_csv_file}\")\n",
    "output_relative_path = os.path.join(relative_path,\"02_Output\")\n",
    "output_csv_relative_path = os.path.join(output_relative_path,\"csv\")\n",
    "log_file_path = os.path.join(relative_path,\"logs\",f\"cms_logs_{datetime_today}.log\")\n",
    "\n",
    "print(f\"url                         = {cms_url}\")\n",
    "print(\"     \")\n",
    "print(f\"relative_path               = {relative_path}\")\n",
    "print(f\"temp_file_path              = {temp_file_path}\")\n",
    "print(f\"csv_file_path               = {csv_file_path}\")\n",
    "print(f\"previous_csv_file_path      = {previous_csv_file_path}\")\n",
    "print(f\"output_relative_path        = {output_relative_path}\")\n",
    "print(f\"output_csv_relative_path   = {output_csv_relative_path}\")\n",
    "print(f\"log_file_path               = {log_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(os.path.join(relative_path,\"01_Source\",\"csv\")):\n",
    "    os.makedirs(os.path.join(relative_path,\"01_Source\",\"csv\"))\n",
    "if not os.path.exists(os.path.join(relative_path,\"01_Source\",\"temp\")):\n",
    "    os.makedirs(os.path.join(relative_path,\"01_Source\",\"temp\"))\n",
    "if not os.path.exists(os.path.join(relative_path,\"logs\")):\n",
    "    os.makedirs(os.path.join(relative_path,\"logs\"))\n",
    "if not os.path.exists(output_csv_relative_path):\n",
    "    os.makedirs(output_csv_relative_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start Spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"Hospitals\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_logger(log_file_path):\n",
    "    # Create a logger\n",
    "    logger = logging.getLogger(__name__)\n",
    "    logger.setLevel(logging.INFO)  # Set the logging level\n",
    "\n",
    "    # Create file handler and console handler\n",
    "    file_handler = logging.FileHandler(log_file_path)\n",
    "    console_handler = logging.StreamHandler(sys.stdout)\n",
    "\n",
    "    # Create formatter\n",
    "    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "\n",
    "    # Add formatter to handlers\n",
    "    file_handler.setFormatter(formatter)\n",
    "    console_handler.setFormatter(formatter)\n",
    "\n",
    "    # Add handlers to logger\n",
    "    logger.addHandler(file_handler)\n",
    "    logger.addHandler(console_handler)\n",
    "    return logger\n",
    "\n",
    "logger = setup_logger(log_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_json_from_url(url, temp_file_path):\n",
    "    try:\n",
    "        # 1. Download the JSON file\n",
    "        logger.info(f\"Downloading JSON file from: {url}\")\n",
    "\n",
    "        response = urllib.request.urlopen(url)\n",
    "        data = response.read()\n",
    "        content = data.decode('utf-8')\n",
    "\n",
    "        with open(temp_file_path, \"w\") as f:\n",
    "            f.write(content)\n",
    "        logger.info(f\"Successfully written JSON to: {temp_file_path}\")\n",
    "\n",
    "        # 2. Read the JSON file into a PySpark DataFrame\n",
    "        logger.info(\"Reading JSON data into Spark DataFrame...\")\n",
    "        df = spark.read.json(temp_file_path)\n",
    "\n",
    "        #         # \n",
    "        # os.remove(temp_file_path)\n",
    "        # logger.info(\"Successfully deleted the temporary file\")\n",
    "\n",
    "        logger.info(\"Successfully read JSON data.\")\n",
    "        return df\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"An error occurred: {e}\")\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = read_json_from_url(cms_url,temp_file_path)\n",
    "data.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data_and_write_to_csv(df, output_path):\n",
    "    try:\n",
    "        logger.info(\"Transforming and filtering items data for 'Hospitals'..\")\n",
    "        df = (df.withColumn(\"theme\",explode(\"theme\"))                                 ## explode theme column\n",
    "                .where(col(\"theme\")==\"Hospitals\")                                     ## filter for where them == Hospitals\n",
    "                .withColumn(\"distribution\", explode(\"distribution\"))                  ## Explode distribution column\n",
    "                .select(\"*\", \"distribution.downloadURL\")                              ## extract downloadURL column\n",
    "                .withColumn(\"current_timestamp\", current_timestamp())                 ## Add current time stamp\n",
    "\n",
    "                .withColumn(\"modified\", col(\"modified\").cast(DateType()))             ## convert modified column to  DateType\n",
    "                \n",
    "                .select(\"identifier\",\"description\",\"issued\",\"landingPage\",\"modified\",\"downloadURL\",\"released\",\"theme\",\"title\",\"current_timestamp\")\n",
    "        )\n",
    "\n",
    "        logger.info(f\"Repartitioning data and saving as as csv\")\n",
    "        df.repartition(1).toPandas().to_csv(output_path,index=False, sep=\",\")\n",
    "        logger.info(f\"Data saved as csv to:{output_path}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"An error occurred: {e}\")\n",
    "        raise\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_csv = process_data_and_write_to_csv(data, csv_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Data that has been modified since last yesterday "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_modified_CSVs(df, previous_csv_path, modified=True):\n",
    "    logger.info(\"Processing data to get list of recently modified data..\")\n",
    "    try: \n",
    "        if not modified:\n",
    "            logger.info(\"mdified set to False\") \n",
    "            logger.info(\"Reading previous day csv file..\") \n",
    "            df_old = spark.read.csv(previous_csv_path, header=True, sep=\",\")\n",
    "            df_old= (df_old.withColumn(\"current_timestamp\", col(\"current_timestamp\").cast(StringType()))\n",
    "                            .withColumn(\"current_timestamp\", (col(\"current_timestamp\").substr(1,10)).cast(DateType()))\n",
    "                            .select(\"current_timestamp\"))\n",
    "            date_previous_day_date = [row[\"current_timestamp\"] for row in df_old.collect()][0]\n",
    "\n",
    "            df = (df.where(col(\"modified\")>date_previous_day_date)\n",
    "                    .select(\"identifier\",\"downloadURL\"))\n",
    "            logger.info(\"Extracted list of recently modified identifiers since previous day\")\n",
    "\n",
    "        else:\n",
    "            logger.info(\"modified set to True\") \n",
    "            df = df.select(\"identifier\",\"downloadURL\")\n",
    "            \n",
    "            logger.info(\"Extracted list of recently modified identifiers since previous day\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"An error occurred: {e}\")\n",
    "        raise\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_csv = get_all_modified_CSVs(data_csv, previous_csv_file_path, modified=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download CSVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_csv_file(url, save_path, logger):\n",
    "    try:\n",
    "        logger.info(f\"Downloading CSV file from {url} to {save_path}\")\n",
    "        urllib.request.urlretrieve(url, save_path)\n",
    "        logger.info(f\"Successfully downloaded file to {save_path}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error downloading file from {url}: {e}\")\n",
    "        raise  # Re-raise to be caught in process_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_csv_and_save(df_csv, output_relative_path):\n",
    "    logger.info(\"Starting download of csv files..\")\n",
    "    df_id = df_csv.select(\"identifier\")\n",
    "    identifier_list = [row[\"identifier\"] for row in df_id.collect()]\n",
    "    df_urls = df_csv.select(\"downloadURL\")\n",
    "    download_url_list = [row[\"downloadURL\"] for row in df_urls.collect()]\n",
    "\n",
    "    try:\n",
    "        downloaded_files = []\n",
    "        for id,url in zip(identifier_list,download_url_list):\n",
    "            logger.info(f\"Making dowloading csv for Hospital ID: {id} \")\n",
    "            logger.info(f\"url: {url} \")\n",
    "\n",
    "            file_name = os.path.splitext(url.split(\"/\")[-1])[0]\n",
    "            \n",
    "            output_path = os.path.join(output_relative_path,f\"{file_name}__{id}__{datetime_today}.csv\")\n",
    "\n",
    "            ## Download the csv file\n",
    "            download_csv_file(url, output_path, logger)\n",
    "            downloaded_files.append(output_path)\n",
    "            logger.info(f\"Downloaded csv file for Hospital ID: {id} \")\n",
    "\n",
    "\n",
    "        # Read all downloaded CSV files into a single Spark DataFrame\n",
    "        # logger.info(\"Reading downloaded CSV files into a Spark DataFrame\")\n",
    "        # df = spark.read.csv(os.path.join(output_relative_path,\"*.csv\"),inferSchema=True, header=True)\n",
    "        # logger.info(\"Successfully read downloaded CSV files into a Spark DataFrame\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        logger.error(f\"An error occurred: {e}\")\n",
    "        raise\n",
    "\n",
    "    return #df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_csv_and_save(df_csv, output_csv_relative_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get test csv data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = spark.read.csv(os.path.join(output_csv_relative_path,\"ASCQR_OAS_CAHPS_STATE__x663-bwbj__2025-04-22_003308.csv\"),inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transform Data and Save csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_data(df, output_path, logger):\n",
    "    try:\n",
    "        columns = df.columns\n",
    "        logger.info(\"Begin transformations..\")\n",
    "        \n",
    "        # Create a list of new column names by replacing spaces with underscores\n",
    "        logger.info(\"Removing special characters and making column names snake case..\")\n",
    "        new_columns = [re.sub(r\"[^0-9a-zA-Z_]+\", \"_\", c.lower().replace(\" \", \"_\")) for c in columns]\n",
    "        \n",
    "        # Use the `toDF` function to rename the columns\n",
    "        df_renamed = df.toDF(*new_columns)\n",
    "\n",
    "        logger.info(\"Saving csv file\")\n",
    "        data.repartition(1).toPandas().to_csv(output_path)\n",
    "        logger.info(f\"CSV file saved to {output_path}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"An error occurred: {e}\")\n",
    "        raise\n",
    "    return df_renamed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_output_path = os.path.join(output_csv_relative_path,\"ASCQR_OAS_CAHPS_STATE__x663-bwbj__2025-04-22_003308.csv\")\n",
    "data_final = transform_data(df_test, final_output_path, logger)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pyspark",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
